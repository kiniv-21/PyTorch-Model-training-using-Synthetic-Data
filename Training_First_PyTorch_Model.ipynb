{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM/mr1BN0uGy5B+kFRK/Gi7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kiniv-21/PyTorch-Model-training-using-Synthetic-Data/blob/main/Training_First_PyTorch_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Dpwk-4Yl3o4s"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('fivethirtyeight')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "true_b = 1\n",
        "true_w = 2\n",
        "N = 100\n",
        "\n",
        "# Data Generation\n",
        "np.random.seed(42)\n",
        "X = np.random.rand(N,1)\n",
        "epsilon = .1 * np.random.randn(N,1)\n",
        "y = true_b + true_w * X + epsilon"
      ],
      "metadata": {
        "id": "zWjCg_tm4HTH"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train/ Validation Split\n",
        "Splitting synthetic data into train and validation sets, shuffling the array of indices and using the first 80 shuffled points for training"
      ],
      "metadata": {
        "id": "2ofhyUed5XUK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Shuffling Indices\n",
        "idx = np.arange(N)\n",
        "np.random.shuffle(idx)\n",
        "\n",
        "# USes the first 80 random indices for train\n",
        "train_idx = idx[:int(N*.8)]\n",
        "val_idx = idx[int(N*.8)]\n",
        "\n",
        "#Generates train and validation sets\n",
        "X_train,y_train = X[train_idx],y[train_idx]\n",
        "X_val, y_val = X[val_idx],y[val_idx]"
      ],
      "metadata": {
        "id": "qmyFaE3S4gIE"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig,ax = plt.subplots(1,2,figsize=(12,4))\n",
        "ax[0].scatter(X_train, y_train)\n",
        "ax[0].set_xlabel('X')\n",
        "ax[0].set_ylabel('y')\n",
        "ax[0].set_ylim([1, 3])\n",
        "ax[0].set_title('Generated Data - Train')\n",
        "ax[1].scatter(X_val, y_val, c='r')\n",
        "ax[1].set_xlabel('X')\n",
        "ax[1].set_ylabel('y')\n",
        "ax[1].set_ylim([1, 3])\n",
        "ax[1].set_title('Generated Data - Validation')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 342
        },
        "id": "umWsG8AL6R9s",
        "outputId": "db18e240-74f6-4b58-b551-e59cd14800f4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Generated Data - Validation')"
            ]
          },
          "metadata": {},
          "execution_count": 5
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 864x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PyTorch: tensors, tensors, tensors"
      ],
      "metadata": {
        "id": "lr2pkrbC_fjC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "psZ7BnMp_Rre"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Numpy, you may have an array that has three dimensions, right? That is, technically speaking, a tensor.\n",
        "\n",
        "A scalar (a single number) has zero dimensions, a vector has one dimension, a matrix has two dimensions and a tensor has three or more dimensions. That’s it!\n",
        "\n",
        "But, to keep things simple, it is commonplace to call vectors and matrices tensors as well — so, from now on, everything is either a scalar or a tensor.\n",
        "\n",
        "You can create tensors in PyTorch pretty much the same way you create arrays in Numpy. Using tensor() you can create either a scalar or a tensor.\n",
        "\n",
        "PyTorch's tensors have equivalent functions as its Numpy counterparts, like: ones(), zeros(), rand(), randn() and many more."
      ],
      "metadata": {
        "id": "6pVNbQbWKjdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scalar = torch.tensor(3.14159)\n",
        "vector = torch.tensor([1, 2, 3])\n",
        "matrix = torch.ones((2, 3), dtype=torch.float)\n",
        "tensor = torch.randn((2, 3, 4), dtype=torch.float)\n",
        "\n",
        "print(scalar)\n",
        "print(vector)\n",
        "print(matrix)\n",
        "print(tensor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CFO4tVqiKhXd",
        "outputId": "ac6ef5a7-5cb6-405d-fcc9-94a9fd2d24b1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(3.1416)\n",
            "tensor([1, 2, 3])\n",
            "tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.]])\n",
            "tensor([[[ 1.7944,  0.3652, -0.1384,  0.9156],\n",
            "         [ 1.0132,  0.3741, -1.1390, -0.1468],\n",
            "         [-0.5580,  1.0622,  0.1158,  1.0490]],\n",
            "\n",
            "        [[ 1.7118, -0.2548,  0.0790,  1.2846],\n",
            "         [ 0.0894,  0.3035, -1.0248, -0.4088],\n",
            "         [ 1.0661, -0.7540,  2.3139,  2.9268]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can get the shape of a tensor using its size() method or its shape attribute."
      ],
      "metadata": {
        "id": "bPlHHMFDLTHW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(tensor.size(), tensor.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2zsePS_XLR1c",
        "outputId": "0e0e5e80-e6a2-4616-b7cd-92e1e6bbb4be"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 3, 4]) torch.Size([2, 3, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also reshape a tensor using its reshape() or view() methods.\n",
        "\n",
        "Beware: these methods create a new tensor with the desired shape that shares the underlying data with the original tensor!"
      ],
      "metadata": {
        "id": "KUKTEKISLXbY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_tensor1 = tensor.reshape(2, -1)\n",
        "new_tensor2 = tensor.view(2, -1)\n",
        "print(new_tensor1.shape, new_tensor2.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZUd_xLG0LV_b",
        "outputId": "953cceac-ccac-4f3b-99a1-5df48c644e8b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 12]) torch.Size([2, 12])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you want to copy all data for real, that is, duplicate it in memory, you should use either its new_tensor() or clone() methods."
      ],
      "metadata": {
        "id": "Wt1t-bLWLa7C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading Data, Devices and CUDA\n",
        "”How do we go from Numpy’s arrays to PyTorch’s tensors”, you ask?\n",
        "\n",
        "That’s what as_tensor() is good for. It returns a CPU tensor, though.\n",
        "\n",
        "You can also easily cast it to a lower precision (32-bit float) using float()."
      ],
      "metadata": {
        "id": "2p69hdRGLd2Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Our data was in Numpy arrays, but we need to transform them into PyTorch's Tensors\n",
        "x_train_tensor = torch.as_tensor(X_train).float()\n",
        "y_train_tensor = torch.as_tensor(y_train).float()\n",
        "\n",
        "print(type(X_train), type(x_train_tensor))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MZS4NzxsLZZq",
        "outputId": "c1e8dfc3-5246-4ad3-b80d-6440ba56132b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'numpy.ndarray'> <class 'torch.Tensor'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "“But I want to use my fancy GPU…”, you say.\n",
        "\n",
        "No worries, that’s what to() is good for. It sends your tensor to whatever device you specify, including your GPU (referred to as cuda or cuda:0).\n",
        "\n",
        "“What if I want my code to fallback to CPU if no GPU is available?”, you may be wondering…\n",
        "\n",
        "PyTorch got your back once more — you can use cuda.is_available() to find out if you have a GPU at your disposal and set your device accordingly."
      ],
      "metadata": {
        "id": "tW93ZwcdLoZY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# Our data was in Numpy arrays, but we need to transform them into PyTorch's Tensors\n",
        "x_train_tensor = torch.as_tensor(X_train).float().to(device)\n",
        "y_train_tensor = torch.as_tensor(y_train).float().to(device)\n",
        "\n",
        "print(type(X_train), type(x_train_tensor))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kTBeWaIvLh6L",
        "outputId": "9669b0ca-2f91-4409-e48f-f56087daca70"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'numpy.ndarray'> <class 'torch.Tensor'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4lsYIvjQDE7O"
      },
      "source": [
        "If you compare the **types** of both variables, you’ll get what you’d expect: `numpy.ndarray` for the first one and `torch.Tensor` for the second one.\n",
        "\n",
        "But where does your nice tensor “live”? In your CPU or your GPU? You can’t say… but if you use PyTorch’s **type()**, it will reveal its **location** — `torch.cuda.FloatTensor` — a GPU tensor in this case."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_train_tensor.type())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8BfqaDJwLqcr",
        "outputId": "068422ce-5d53-4185-da45-3fd66edd82c7"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.cuda.FloatTensor\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also go the other way around, turning tensors back into Numpy arrays, using numpy(). It should be easy as x_train_tensor.numpy() but…"
      ],
      "metadata": {
        "id": "qP2Az_j-L6Vk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_tensor.numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "id": "1rGE1eBLL6xq",
        "outputId": "0ac963d5-20d5-442f-e7b5-d7383fb409fe"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-1f2e1ca5ecef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx_train_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "3wvbjO8QL_Wk"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNKoo-n0DlUP"
      },
      "source": [
        "Unfortunately, Numpy **cannot** handle GPU tensors… you need to make them CPU tensors first using [**cpu()**](https://bit.ly/2OSC1Th)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_tensor.cpu().numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OHCD1U29L8Xa",
        "outputId": "ea27eef4-c094-4b54-eb88-f926422fbc6d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.77127033],\n",
              "       [0.06355835],\n",
              "       [0.86310345],\n",
              "       [0.02541913],\n",
              "       [0.7319939 ],\n",
              "       [0.07404465],\n",
              "       [0.19871569],\n",
              "       [0.31098232],\n",
              "       [0.47221494],\n",
              "       [0.96958464],\n",
              "       [0.12203824],\n",
              "       [0.77513283],\n",
              "       [0.802197  ],\n",
              "       [0.72960615],\n",
              "       [0.09767211],\n",
              "       [0.18485446],\n",
              "       [0.15601864],\n",
              "       [0.02058449],\n",
              "       [0.9868869 ],\n",
              "       [0.6232981 ],\n",
              "       [0.7080726 ],\n",
              "       [0.5979    ],\n",
              "       [0.9218742 ],\n",
              "       [0.63755745],\n",
              "       [0.2809345 ],\n",
              "       [0.25877997],\n",
              "       [0.11959425],\n",
              "       [0.7290072 ],\n",
              "       [0.94888556],\n",
              "       [0.60754484],\n",
              "       [0.5612772 ],\n",
              "       [0.4937956 ],\n",
              "       [0.18182497],\n",
              "       [0.27134904],\n",
              "       [0.96990985],\n",
              "       [0.21233912],\n",
              "       [0.1834045 ],\n",
              "       [0.8661761 ],\n",
              "       [0.37454012],\n",
              "       [0.29122913],\n",
              "       [0.80839735],\n",
              "       [0.05808361],\n",
              "       [0.83244264],\n",
              "       [0.54269606],\n",
              "       [0.77224475],\n",
              "       [0.88721275],\n",
              "       [0.08849251],\n",
              "       [0.04522729],\n",
              "       [0.59241456],\n",
              "       [0.684233  ],\n",
              "       [0.7132448 ],\n",
              "       [0.03438852],\n",
              "       [0.601115  ],\n",
              "       [0.81546146],\n",
              "       [0.4401525 ],\n",
              "       [0.32518333],\n",
              "       [0.785176  ],\n",
              "       [0.76078504],\n",
              "       [0.4951769 ],\n",
              "       [0.19967379],\n",
              "       [0.9507143 ],\n",
              "       [0.29214466],\n",
              "       [0.13949387],\n",
              "       [0.31171107],\n",
              "       [0.7068573 ],\n",
              "       [0.11586906],\n",
              "       [0.35846573],\n",
              "       [0.00552212],\n",
              "       [0.19598286],\n",
              "       [0.89482737],\n",
              "       [0.45606998],\n",
              "       [0.52475643],\n",
              "       [0.14092423],\n",
              "       [0.06505159],\n",
              "       [0.17052412],\n",
              "       [0.8287375 ],\n",
              "       [0.32533032],\n",
              "       [0.93949896],\n",
              "       [0.33089802],\n",
              "       [0.36636186]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we know how to handle tensors, we can use them to build a..."
      ],
      "metadata": {
        "id": "wYEndthRMFlq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ],
      "metadata": {
        "id": "RmKIeUGYMGg0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In PyTorch, a dataset is represented by a regular Python class that inherits from the Dataset class. You can think of it as a kind of a Python list of tuples, each tuple corresponding to one point (features, label)."
      ],
      "metadata": {
        "id": "4LqW2PL9MIm0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tensor Dataset"
      ],
      "metadata": {
        "id": "7OYCZ8FvMKuI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If a dataset is nothing else but a couple of tensors, we can use PyTorch’s TensorDataset class.\n",
        "\n",
        "Let's create some tensors out of our training data and use them to build a dataset!"
      ],
      "metadata": {
        "id": "OzL08-mUMPIP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset\n",
        "\n",
        "x_train_tensor = torch.as_tensor(X_train).float()\n",
        "y_train_tensor = torch.as_tensor(y_train).float()\n",
        "\n",
        "train_data = TensorDataset(x_train_tensor, y_train_tensor)\n",
        "print(train_data[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u6KY4x2DMBXb",
        "outputId": "7e65a5de-10f2-4abb-ca3f-072875c54eaf"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(tensor([0.7713]), tensor([2.4745]))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "Did you notice we built our **training tensors** out of Numpy arrays but we **did not send them to a device**? So, they are **CPU** tensors now! **Why**?\n",
        "\n",
        "We **don’t want our whole training data to be loaded into GPU tensors** because **it takes up space** in our precious **graphics card’s RAM**.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "eXTv8-pJMUnp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "OK, fine, but then again, why are we building a dataset anyway? We’re doing it because we want to use a…"
      ],
      "metadata": {
        "id": "ckUUWJhEMZvf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DataLoader, splitting your data into mini-batches"
      ],
      "metadata": {
        "id": "F1jZhFwfMbN6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use PyTorch’s DataLoader class for this job. We tell it which dataset to use (the one we just built in the previous section), the desired mini-batch size and if we’d like to shuffle it or not. That’s it!\n",
        "\n",
        "Our loader will behave like an iterator, so we can loop over it and fetch a different mini-batch every time."
      ],
      "metadata": {
        "id": "oz3gXo6DMeaP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_loader = DataLoader(dataset=train_data, batch_size=16, shuffle=True)"
      ],
      "metadata": {
        "id": "Ago2_v-gMRE6"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To retrieve a mini-batch, one can simply run the command below — it will return a list containing two tensors, one for the features, another one for the labels."
      ],
      "metadata": {
        "id": "i2j9vx3wMiA1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_batch, y_train_batch = next(iter(train_loader))\n",
        "# we send the mini-batch to a device\n",
        "x_train_batch = x_train_batch.to(device)\n",
        "y_train_batch = y_train_batch.to(device)"
      ],
      "metadata": {
        "id": "jYzxZa-TMg9a"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cool, we loaded our training data into a TensorDataset, and used a DataLoader to generate mini-batches. Now it's time to handle other type of tensors..."
      ],
      "metadata": {
        "id": "YjzPJC4ZMnCh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating Tensor for Parameters"
      ],
      "metadata": {
        "id": "StE0hJpVModN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What distinguishes a *tensor* used for *data* — like the ones we’ve just created — from a **tensor** used as a (*trainable*) **parameter/weight**?\n",
        "\n",
        "The latter tensors require the **computation of its gradients**, so we can **update** their values (the parameters’ values, that is). That’s what the **`requires_grad=True`** argument is good for. It tells PyTorch we want it to compute gradients for us.\n",
        "\n",
        "---\n",
        "\n",
        "<h2><b><i>A tensor for a learnable parameter requires gradient!</i></b></h2>\n",
        "\n",
        "---\n",
        "\n",
        "You may be tempted to create a simple tensor for a parameter and, later on, send it to your chosen device, as we did with our data, right?\n",
        "\n",
        "Actually, you should **assign** tensors to a **device** at the moment of their **creation** to avoid unexpected behaviors..."
      ],
      "metadata": {
        "id": "OfCaToZ7MtjB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We can specify the device at the moment of creation - RECOMMENDED!\n",
        "torch.manual_seed(42)\n",
        "b = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
        "w = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
        "print(b, w)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kyJu0f4FMk36",
        "outputId": "93f3a4f8-bfb7-4259-a00b-cc3b204965db"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.1940], device='cuda:0', requires_grad=True) tensor([0.1391], device='cuda:0', requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "That works fine for a tiny model like ours, but it isn't practical at all if the model has many parameters. Luckily, we can replace a bunch of individual parameters for a single Linear layer."
      ],
      "metadata": {
        "id": "CkHqUjpgMw4T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "layer = nn.Linear(1, 1)\n",
        "print(layer.bias, layer.weight)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gaF_d9G1Mvs6",
        "outputId": "a52f0e97-a703-4f83-8ef1-beff89d53660"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([0.8300], requires_grad=True) Parameter containing:\n",
            "tensor([[0.7645]], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Layers"
      ],
      "metadata": {
        "id": "ek9XnDEnM0Z3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A **Linear** model can be seen as a **layer** in a neural network.\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"https://raw.githubusercontent.com/dvgodoy/AnalyticsVidhya_DataHour_PyTorch/main/images/layer.png\" width=\"50%\" height=\"50%\">\n",
        "</p>\n",
        "\n",
        "In the example above, the **hidden layer** would be `nn.Linear(3, 5)` and the **output layer** would be `nn.Linear(5, 1)`.\n",
        "\n",
        "\n",
        "There are **MANY** different layers that can be uses in PyTorch:\n",
        "- [Convolution Layers](https://pytorch.org/docs/stable/nn.html#convolution-layers)\n",
        "- [Pooling Layers](https://pytorch.org/docs/stable/nn.html#pooling-layers)\n",
        "- [Padding Layers](https://pytorch.org/docs/stable/nn.html#padding-layers)\n",
        "- [Non-linear Activations](https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity)\n",
        "- [Normalization Layers](https://pytorch.org/docs/stable/nn.html#normalization-layers)\n",
        "- [Recurrent Layers](https://pytorch.org/docs/stable/nn.html#recurrent-layers)\n",
        "- [Transformer Layers](https://pytorch.org/docs/stable/nn.html#transformer-layers)\n",
        "- [Linear Layers](https://pytorch.org/docs/stable/nn.html#linear-layers)\n",
        "- [Dropout Layers](https://pytorch.org/docs/stable/nn.html#dropout-layers)\n",
        "- [Sparse Layers (embbedings)](https://pytorch.org/docs/stable/nn.html#sparse-layers)\n",
        "- [Vision Layers](https://pytorch.org/docs/stable/nn.html#vision-layers)\n",
        "- [DataParallel Layers (multi-GPU)](https://pytorch.org/docs/stable/nn.html#dataparallel-layers-multi-gpu-distributed)\n",
        "- [Flatten Layer](https://pytorch.org/docs/stable/nn.html#flatten)\n",
        "\n",
        "We have just used a **Linear** layer."
      ],
      "metadata": {
        "id": "BANsnNGlM5ru"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Moreover, if we're sticking with a simple and straightforward model where the output of each layer feeds directly into the next, we can line them up using a sequential model!"
      ],
      "metadata": {
        "id": "vGERbrhmNFFZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2><b><i>Run-of-the-mill layers? Sequential model!</b></i></h2>\n",
        "\n",
        "For **straightforward models**, that use **run-of-the-mill layers**, where the output of a layer is sequentially fed as an input to the next, we can use a, er… [**Sequential**](https://bit.ly/3hRQTxP) model :-)\n",
        "\n",
        "In our case, we would build a Sequential model with a single argument, that is, the Linear layer we used to train our linear regression. The model would look like this:"
      ],
      "metadata": {
        "id": "OGFDFKoeNHOD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_features = 1\n",
        "n_outputs = 1\n",
        "\n",
        "torch.manual_seed(42)\n",
        "model = nn.Sequential()\n",
        "model.add_module('linear', nn.Linear(n_features, n_outputs, bias=True))\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wh4gBR9sMygZ",
        "outputId": "a586fbdf-5718-4f65-9723-0c86b311fac9"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (linear): Linear(in_features=1, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Device"
      ],
      "metadata": {
        "id": "k2xqp_-BNL9_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**IMPORTANT**: we need to **send our model to the same device where the data is**. If our data is made of GPU tensors, our model must “live” inside the GPU as well."
      ],
      "metadata": {
        "id": "o5TV5T-ENScR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xEwvgRTNNKZa",
        "outputId": "d5945f0d-e860-44b3-ea2c-6e3d530e9c25"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (linear): Linear(in_features=1, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### state_dict"
      ],
      "metadata": {
        "id": "xpwxyHvlNWJ6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **state_dict()** of a given model is simply a Python dictionary that **maps each layer / parameter to its corresponding tensor**. But only **learnable** parameters are included, as its purpose is to keep track of parameters that are going to be updated."
      ],
      "metadata": {
        "id": "hs29osVpNXRU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.state_dict()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-a8EbA1XNUaJ",
        "outputId": "90bd67a2-abf0-43da-9504-2153fcc4caca"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('linear.weight', tensor([[0.7645]], device='cuda:0')),\n",
              "             ('linear.bias', tensor([0.8300], device='cuda:0'))])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.linear.weight"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dyX-wVcONayY",
        "outputId": "563839d7-b90e-4c2d-dea6-7cba6ad3f21c"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[0.7645]], device='cuda:0', requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we know how to create tensors and models that require gradients, let’s see how PyTorch handles them — that’s the role of the…"
      ],
      "metadata": {
        "id": "Zhl_MbcsNeP8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradient Descent in 5 easy steps!"
      ],
      "metadata": {
        "id": "_MEgGYy9NhZY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gradient descent is the most common **optimization algorithm** in Machine Learning and Deep Learning.\n",
        "\n",
        "The purpose of using gradient descent is **to minimize the loss**, that is, **minimize the errors between predictions and actual values** (and sometimes some other term as well).\n",
        "\n",
        "It goes beyond the scope of this tutorial to fully explain how gradient descent works, but I'll cover the **five basic steps** you'd need to go through to compute it, namely:\n",
        "\n",
        "- Step 0: Random initialize parameters / weights\n",
        "- Step 1: Compute model's predictions - forward pass\n",
        "- Step 2: Compute loss\n",
        "- Step 3: Compute the gradients\n",
        "- Step 4: Update the parameters\n",
        "- Step 5: Rinse and repeat!\n",
        "\n",
        "---\n",
        "\n",
        "If you want to learn more about gradient descent, check the following resources:\n",
        "- [**Linear Regression Simulator**](https://www.mladdict.com/linear-regression-simulator), which goes through the very same steps listed here\n",
        "- [**A Visual and Interactive Guide to the Basics of Neural Networks**](http://jalammar.github.io/visual-interactive-guide-basics-neural-networks/)\n",
        "- [**Gradient Descent Algorithms and Its Variants**](https://towardsdatascience.com/gradient-descent-algorithm-and-its-variants-10f652806a3)\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "rADoTiM7NhAJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 0: Initialization\n",
        "\n",
        "Technically, this step is not part of gradient descent, but it is an important step nonetheless.\n",
        "\n",
        "For training a model, you need to **randomly initialize the parameters/weights**.\n",
        "\n",
        "Make sure to *always initialize your random seed* to ensure **reproducibility** of your results. As usual, the random seed is [42](https://en.wikipedia.org/wiki/Phrases_from_The_Hitchhiker%27s_Guide_to_the_Galaxy#Answer_to_the_Ultimate_Question_of_Life,_the_Universe,_and_Everything_(42)), the *least random* of all random seeds one could possibly choose :-)"
      ],
      "metadata": {
        "id": "Doiw2s03Nlvz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 0\n",
        "torch.manual_seed(42)\n",
        "\n",
        "model = nn.Sequential()\n",
        "model.add_module('linear', nn.Linear(n_features, n_outputs, bias=True))\n",
        "model.to(device)\n",
        "print(model.state_dict())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qzg6XeTgNcYp",
        "outputId": "78d3cfd0-8744-4d5d-8ff8-b99d035d8a58"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OrderedDict([('linear.weight', tensor([[0.7645]], device='cuda:0')), ('linear.bias', tensor([0.8300], device='cuda:0'))])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: Compute Model's Predictions\n",
        "\n",
        "This is the **forward pass** - it simply *computes the model's predictions using the current values of the parameters/weights*. At the very beginning, we will be producing really bad predictions, as we started with random values from Step 0.\n",
        "\n",
        "Predictions for a simple linear regression like ours *could* be computed like this, but that's not practical at all..."
      ],
      "metadata": {
        "id": "tLnww_aeNohk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "yhat = model.linear.bias + model.linear.weight * x_train_batch\n",
        "yhat[:3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PtIDJtGyNn55",
        "outputId": "4dc42e67-774a-44bc-873b-deb410ea8f52"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.9186],\n",
              "        [1.4204],\n",
              "        [1.2829]], device='cuda:0', grad_fn=<SliceBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use our model class instead:"
      ],
      "metadata": {
        "id": "iZ4v2iCqNtYq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "yhat = model(x_train_batch)\n",
        "yhat[:3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PswWXIqnNrrZ",
        "outputId": "38e3ac32-6c84-4da9-b4aa-0e7e80b1ba53"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.9186],\n",
              "        [1.4204],\n",
              "        [1.2829]], device='cuda:0', grad_fn=<SliceBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "Even though models have a `forward` method, you should **NOT call the `forward(x)`** method. You should always **call the whole model itself**, as in **`model(x)`** to perform a forward pass and output predictions.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "1ihMM6rsNw8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In PyTorch, models have a [**train()**](https://bit.ly/30VW2Ox) method which, somewhat disappointingly, **does NOT perform a training step**. Its only purpose is to **set the model to training mode**. \n",
        "\n",
        "Why is this important? Some models may use mechanisms like [**Dropout**](https://bit.ly/2X7v5pU), for instance, which have **distinct behaviors in training and evaluation phases**."
      ],
      "metadata": {
        "id": "lpyygMdGNzDI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1\n",
        "# Computes our model's predicted output\n",
        "model.train()\n",
        "yhat = model(x_train_batch)\n",
        "yhat[:3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NucHMpxHNvQ5",
        "outputId": "3f7dc8a6-248c-4ebd-9fa5-970ea399e08e"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.9186],\n",
              "        [1.4204],\n",
              "        [1.2829]], device='cuda:0', grad_fn=<SliceBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is a subtle but fundamental difference between **error** and **loss**. \n",
        "\n",
        "The **error** is the difference between **actual** and **predicted** computed for a single data point.\n",
        "\n",
        "$$\n",
        "\\Large \\text{error}_i = \\hat{y_i} - y_i\n",
        "$$\n",
        "\n",
        "The **loss**, on the other hand, is some sort of **aggregation of errors for a set of data points**.\n",
        "\n",
        "For a regression problem, the **loss** is given by the **Mean Squared Error (MSE)**, that is, the average of all squared differences between **actual values** (y) and **predictions** (a + bx).\n",
        "\n",
        "$$\n",
        "\\large \\text{MSE} = \\frac{1}{N} \\sum_{i=1}^N{\\text{error}_i}^2\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\large \\text{MSE} = \\frac{1}{N} \\sum_{i=1}^N{(\\hat{y_i} - y_i)}^2\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\large \\text{MSE} = \\frac{1}{N} \\sum_{i=1}^N{(b + w x_i - y_i)}^2\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "It is worth mentioning that, if we **compute the loss** using:\n",
        "- **all points** in the training set (N), we are performing a **batch** gradient descent\n",
        "- a **single point** at each time, it would be a **stochastic** gradient descent\n",
        "- anything else (n) **in-between 1 and N** characterizes a **mini-batch** gradient descent\n",
        "\n",
        "---\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"https://raw.githubusercontent.com/dvgodoy/AnalyticsVidhya_DataHour_PyTorch/main/images/gradient_descent.png\">\n",
        "</p>"
      ],
      "metadata": {
        "id": "rOB2vmaEN2fJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# How wrong is our model? That's the error! \n",
        "error = (yhat - y_train_batch)\n",
        "\n",
        "# It is a regression, so it computes mean squared error (MSE)\n",
        "loss = (error ** 2).mean()\n",
        "\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8tRDC_k9N1Fp",
        "outputId": "26f9eede-b2cd-4bb3-a63e-c04dc6a6987d"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.7127, device='cuda:0', grad_fn=<MeanBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loss: aggregating errors into a single value"
      ],
      "metadata": {
        "id": "WuRkezDnPjny"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Computing losses manually isn't great. Luckily, PyTorch got us covered once again. There are many [loss functions](https://pytorch.org/docs/stable/nn.html#loss-functions) to choose from, depending on the task at hand. Since ours is a regression, we are using the [Mean Square Error (MSE)](https://bit.ly/3hNYn4R) loss.\n",
        "\n",
        "---\n",
        "\n",
        "Notice that `nn.MSELoss` actually **creates a loss function** for us — **it is NOT the loss function itself**. Moreover, you can specify a **reduction method** to be applied, that is, **how do you want to aggregate the results for individual points** — you can average them (reduction=’mean’) or simply sum them up (reduction=’sum’).\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "GLUc9xWmPkUx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defines a MSE loss function\n",
        "loss_fn = nn.MSELoss(reduction='mean')\n",
        "\n",
        "loss_fn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nX8-7_VOPh_o",
        "outputId": "a4ed37b0-d69d-4904-c5e4-7d05ee480616"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MSELoss()"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We then **use** the created loss function to compute the loss given our **predictions** and our **labels**."
      ],
      "metadata": {
        "id": "Sy4M9GZ4PpOz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss = loss_fn(yhat, y_train_batch)\n",
        "loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mo8Hw4cWPn4h",
        "outputId": "2d713ccf-d54f-40b8-d8bc-6d0a977405f4"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.7127, device='cuda:0', grad_fn=<MseLossBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: Compute the Gradients"
      ],
      "metadata": {
        "id": "zNdru5PfPsVX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A **gradient** is a **partial derivative** — *why partial*? Because one computes it with respect to (w.r.t.) a **single parameter**. We have two parameters, **b** and **w**, so we must compute two partial derivatives.\n",
        "\n",
        "A **derivative** tells you *how much* **a given quantity changes** when you *slightly* vary some **other quantity**. In our case, how much does our **MSE** **loss** change when we vary **each one of our two parameters**?\n",
        "\n",
        "The *right-most* part of the equations below is what you usually see in implementations of gradient descent for a simple linear regression. In the **intermediate step**, I show you **all elements** that pop-up from the application of the [chain rule](https://en.wikipedia.org/wiki/Chain_rule), so you know how the final expression came to be.\n",
        "\n",
        "---\n",
        "\n",
        "<h3><i><b>Gradient = how much the LOSS changes if ONE parameter changes a little bit!</b></i></h3>\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "WkS8aec5PvvR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Gradients**:\n",
        "\n",
        "$$\n",
        "\\large \\frac{\\partial{\\text{MSE}}}{\\partial{b}} = \\frac{\\partial{\\text{MSE}}}{\\partial{\\hat{y_i}}} \\frac{\\partial{\\hat{y_i}}}{\\partial{b}} = \\frac{1}{N} \\sum_{i=1}^N{2(b + w x_i - y_i)} = 2 \\frac{1}{N} \\sum_{i=1}^N{(\\hat{y_i} - y_i)}\n",
        "$$ \n",
        "\n",
        "$$\n",
        "\\large \\frac{\\partial{\\text{MSE}}}{\\partial{w}} = \\frac{\\partial{\\text{MSE}}}{\\partial{\\hat{y_i}}} \\frac{\\partial{\\hat{y_i}}}{\\partial{w}} = \\frac{1}{N} \\sum_{i=1}^N{2(b + w x_i - y_i) \\cdot x_i} = 2 \\frac{1}{N} \\sum_{i=1}^N{x_i (\\hat{y_i} - y_i)}\n",
        "$$\n"
      ],
      "metadata": {
        "id": "WFsmeNFKP0YK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Computes gradients for both \"b\" and \"w\" parameters\n",
        "b_grad = 2 * error.mean()\n",
        "w_grad = 2 * (x_train_batch * error).mean()\n",
        "print(b_grad, w_grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w4XytsUYPq7A",
        "outputId": "ba4bbaa3-d269-4fc7-9368-bdd0dfde20d4"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(-1.4898, device='cuda:0', grad_fn=<MulBackward0>) tensor(-0.9395, device='cuda:0', grad_fn=<MulBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Autograd, your companion for all your gradient needs!"
      ],
      "metadata": {
        "id": "lnCtr7FWP4Mr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Autograd is PyTorch’s *automatic differentiation package*. Thanks to it, we **don’t need to worry** about partial derivatives, chain rule or anything like it.\n",
        "\n",
        "<h2><b><i>Computing gradients manually?! No way! Backward!</b></i></h2>\n"
      ],
      "metadata": {
        "id": "TNXJarO2P6Mm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### backward"
      ],
      "metadata": {
        "id": "EWGkgv0NP8lf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, how do we tell PyTorch to do its thing and **compute all gradients**? That’s what [**backward()**](https://bit.ly/3eV9Dub) is good for.\n",
        "\n",
        "Do you remember the **starting point** for **computing the gradients**? It was the **loss**, as we computed its partial derivatives w.r.t. our parameters. Hence, we need to invoke the `backward()` method from the corresponding Python variable, like, `loss.backward()`."
      ],
      "metadata": {
        "id": "3ZWxMTrgP8bv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "yhat = model(x_train_batch)\n",
        "loss = loss_fn(yhat, y_train_batch)\n",
        "# Step 3    \n",
        "# No more manual computation of gradients! \n",
        "loss.backward()"
      ],
      "metadata": {
        "id": "8eEKQyoVPuHh"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### grad / zero_"
      ],
      "metadata": {
        "id": "aWAce5K4QBsY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "What about the **actual values** of the **gradients**? We can inspect them by looking at the [**grad**](https://bit.ly/3fYtNFa) **attribute** of a tensor."
      ],
      "metadata": {
        "id": "5E4oyX_XQC-_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.linear.weight.grad, model.linear.bias.grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dniVXo6qQAd3",
        "outputId": "7237d827-5de8-4207-eddd-e93fdcc5cd51"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[-0.9395]], device='cuda:0'), tensor([-1.4898], device='cuda:0'))"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you check the method’s documentation, it clearly states that **gradients are accumulated**. \n",
        "\n",
        "So, every time we use the **gradients** to **update** the parameters, we need to **zero the gradients afterwards**. And that’s what [**zero_()**](https://pytorch.org/docs/stable/generated/torch.Tensor.zero_.html#torch.Tensor.zero_) is good for.\n",
        "\n",
        "---\n",
        "\n",
        "*In PyTorch, every method that **ends** with an **underscore (_)** makes changes **in-place**, meaning, they will **modify** the underlying variable.*\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "68Cjf34EQLXn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.linear.weight.grad.zero_(), model.linear.bias.grad.zero_()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MSSEtHj2QJIm",
        "outputId": "e6b3ce52-dd4c-40de-cd5e-cc30a6d34bae"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[0.]], device='cuda:0'), tensor([0.], device='cuda:0'))"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4: Update the Parameters"
      ],
      "metadata": {
        "id": "HIOYQpfpQss3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the final step, we **use the gradients to update** the parameters. Since we are trying to **minimize** our **losses**, we **reverse the sign** of the gradient for the update.\n",
        "\n",
        "There is still another parameter to consider: the **learning rate**, denoted by the *Greek letter* **eta** (that looks like the letter **n**), which is the **multiplicative factor** that we need to apply to the gradient for the parameter update."
      ],
      "metadata": {
        "id": "JgFH4do2Qvx7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Parameters**:\n",
        "\n",
        "$$\n",
        "\\large b = b - \\eta \\frac{\\partial{\\text{MSE}}}{\\partial{b}}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\large w = w - \\eta \\frac{\\partial{\\text{MSE}}}{\\partial{w}}\n",
        "$$"
      ],
      "metadata": {
        "id": "4HK_eHiyQsmZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Optimizer:  learning the parameters step-by-step"
      ],
      "metadata": {
        "id": "8kM2oJguQ0Cu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Updating parameters **manually** is probably fine for *two parameters*… but what if we had a **whole lot of them**?! We use one of PyTorch’s **optimizers**, like [SGD](https://pytorch.org/docs/stable/optim.html#torch.optim.SGD) or [Adam](https://pytorch.org/docs/stable/optim.html#torch.optim.Adam).\n",
        "\n",
        "---\n",
        "\n",
        "There are **many** optimizers, **SGD** is the most basic of them and **Adam** is one of the most popular. They achieve the same goal through, literally, **different paths**.\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"https://raw.githubusercontent.com/dvgodoy/AnalyticsVidhya_DataHour_PyTorch/main/images/sgd_adam_paths.png\" width=\"50%\"/>\n",
        "</p>\n",
        "\n",
        "---\n",
        "\n",
        "In the code below, we create a *Stochastic Gradient Descent* (SGD) optimizer to update our parameters **b** and **w**.\n",
        "\n",
        "---\n",
        "\n",
        "Don’t be fooled by the **optimizer’s** name: if we use **all training data** at once for the update the optimizer is performing a **batch** gradient descent, despite of its name.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "yTdHi5YqQ14c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's start with a value of **0.1** (which is a relatively *high value*, as far as learning rates are concerned!)."
      ],
      "metadata": {
        "id": "DOVwxJ1XQ4bx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Learning rate\n",
        "lr = 1e-1\n",
        "# Defines a SGD optimizer to update the parameters\n",
        "optimizer = optim.SGD(model.parameters(), lr=lr)"
      ],
      "metadata": {
        "id": "G8Jm9z42QNY3"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list(model.parameters())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51UKR_iPQ58W",
        "outputId": "515020d4-ffe8-4dc0-d057-bc871f49eaba"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Parameter containing:\n",
              " tensor([[0.7645]], device='cuda:0', requires_grad=True), Parameter containing:\n",
              " tensor([0.8300], device='cuda:0', requires_grad=True)]"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### step / zero_grad"
      ],
      "metadata": {
        "id": "WbWqLm-PQ8kQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "An optimizer takes the **parameters** we want to update, the **learning rate** we want to use (and possibly many other hyper-parameters as well!) and **performs the updates** through its [**`step()`**](https://pytorch.org/docs/stable/generated/torch.optim.Optimizer.step.html#torch.optim.Optimizer.step) method."
      ],
      "metadata": {
        "id": "vWLSQNkfQ-yo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "yhat = model(x_train_batch)\n",
        "loss = loss_fn(yhat, y_train_batch)\n",
        "\n",
        "loss.backward()\n",
        "print(model.linear.weight.grad, model.linear.bias.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q1fWoYzTQ7Um",
        "outputId": "c3bbc0d1-6ab5-4ff2-969b-9eec39fa7658"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.9395]], device='cuda:0') tensor([-1.4898], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer.step()\n",
        "print(model.linear.weight.grad, model.linear.bias.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lr4xWxz1RAaV",
        "outputId": "719ded5a-c270-4336-f020-219aed9d301f"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.9395]], device='cuda:0') tensor([-1.4898], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Besides, we also don’t need to zero the gradients one by one anymore. We just invoke the optimizer’s [**`zero_grad()`**](https://pytorch.org/docs/stable/generated/torch.optim.Optimizer.zero_grad.html#torch.optim.Optimizer.zero_grad) method and that’s it!"
      ],
      "metadata": {
        "id": "YjZp81bFRC27"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer.zero_grad()\n",
        "print(model.linear.weight.grad, model.linear.bias.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fBqICsZARBmF",
        "outputId": "1ce33153-ccf4-44d5-f94a-2a095bd7079e"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.]], device='cuda:0') tensor([0.], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.linear.weight, model.linear.bias)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FrkvU8bGRE4X",
        "outputId": "bb4c8f2e-7498-4cd0-f9e7-3f782ec57ea0"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[0.8585]], device='cuda:0', requires_grad=True) Parameter containing:\n",
            "tensor([0.9790], device='cuda:0', requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "<h2><b><i>\"Choose your learning rate wisely...\"</b></i></h2>\n",
        "\n",
        "<h3><i><b>The learning rate is the single most important hyper-parameter to tune when you are using Deep Learning models!</b></i></h3>\n",
        "\n",
        "What happens if I choose the learning rate **poorly**? Your model may **take too long to train** or **get stuck with a high loss** or, even worse, **diverge into an exploding loss**!\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"https://raw.githubusercontent.com/dvgodoy/AnalyticsVidhya_DataHour_PyTorch/main/images/learning_rates.png\" width=\"50%\">\n",
        "</p>\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "SQHLkXxWRHnk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 5: Rinse and Repeat!"
      ],
      "metadata": {
        "id": "lrY0-BUrRKlu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we use the **updated parameters** to go back to **Step 1** and restart the process.\n",
        "\n",
        "Repeating this process over and over, for **many epochs**, is, in a nutshell, **training** a model.\n",
        "\n",
        "---\n",
        "\n",
        "An **epoch** is complete whenever **every point has been already used once for computing the loss**: \n",
        "- **batch** gradient descent: this is trivial, as it uses all points for computing the loss — **one epoch** is the same as **one update**\n",
        "- **stochastic** gradient descent: **one epoch** means **N updates**\n",
        "- **mini-batch** (of size n): **one epoch** has **N/n updates**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Let's put the previous pieces of code together and loop over many epochs:"
      ],
      "metadata": {
        "id": "pKW_vN5oRKi-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 0.1\n",
        "# Defines number of epochs\n",
        "n_epochs = 200\n",
        "\n",
        "torch.manual_seed(42)\n",
        "\n",
        "n_features = 1\n",
        "n_outputs = 1\n",
        "model = nn.Sequential()\n",
        "model.add_module('linear', nn.Linear(n_features, n_outputs, bias=True))\n",
        "model.to(device)\n",
        "\n",
        "# Defines a MSE loss function\n",
        "loss_fn = nn.MSELoss(reduction='mean')\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    for x_train_batch, y_train_batch in train_loader:\n",
        "        x_train_batch = x_train_batch.to(device)\n",
        "        y_train_batch = y_train_batch.to(device)\n",
        "        # Step 1\n",
        "        yhat = model(x_train_batch)\n",
        "\n",
        "        # Step 2\n",
        "        loss = loss_fn(yhat, y_train_batch)\n",
        "\n",
        "        # Step 3\n",
        "        loss.backward() \n",
        "\n",
        "        # Step 4\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()"
      ],
      "metadata": {
        "id": "AhkeKQ0CRGUm"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.state_dict())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hVz_fkHOROCl",
        "outputId": "6832bdf5-aa17-493a-c8f8-f3a2a8c0d5af"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OrderedDict([('linear.weight', tensor([[1.9684]], device='cuda:0')), ('linear.bias', tensor([1.0219], device='cuda:0'))])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Just keep in mind that, if you **don’t** use batch gradient descent (our example does), you’ll have to write an **inner loop** to perform the **four training steps** for either each **individual point** (**stochastic**) or **n points** (**mini-batch**). We’ll see a mini-batch example later down the line."
      ],
      "metadata": {
        "id": "rHXqjygORQuF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sanity Check"
      ],
      "metadata": {
        "id": "0S3HLjI2RSpn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Just to make sure we haven’t done any mistakes in our code, we can use *Scikit-Learn’s Linear Regression* to fit the model and compare the coefficients."
      ],
      "metadata": {
        "id": "mPSt146LRTeX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sanity Check: do we get the same results as our gradient descent?\n",
        "from sklearn.linear_model import LinearRegression\n",
        "linr = LinearRegression()\n",
        "linr.fit(X_train, y_train)\n",
        "print(linr.intercept_, linr.coef_[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CaXNYAztRPa1",
        "outputId": "77a328db-e732-4d68-8f73-c5921b307bb2"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1.02354075] [1.96896447]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "They **match** — we have a *fully working implementation of linear regression* using PyTorch!"
      ],
      "metadata": {
        "id": "b-EJp8T4Raoe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we managed to successfully run our model and get the **resulting parameters**. Surely enough, they **match** the ones we got in our *Numpy*-only implementation.\n",
        "\n",
        "Let's take a look at the **loss** at the end of the training..."
      ],
      "metadata": {
        "id": "kAZOekDERcjp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZPeJdgM4RWBl",
        "outputId": "36107add-29cd-44d9-8636-766073e356c6"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.0094, device='cuda:0', grad_fn=<MseLossBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What if we wanted to have it as a *Numpy* array? I guess we could just use **numpy()** again, right? (and **cpu()** as well, since our *loss* is in the `cuda` device..."
      ],
      "metadata": {
        "id": "_vfC5Pf9Rhp5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss.cpu().numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "id": "qMs9dq-ARfql",
        "outputId": "609b9d97-69e4-4602-efc4-71f096a78fb3"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-51-58c76a7bac74>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What happened here? Unlike our *data tensors*, the **loss tensor** is actually computing gradients - and in order to use **numpy**, we need to [**detach()**](https://pytorch.org/docs/stable/generated/torch.Tensor.detach.html#torch.Tensor.detach) that tensor from the computation graph first:"
      ],
      "metadata": {
        "id": "tStq_9bYRlDx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss.detach().cpu().numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1LLQ4HhQRjT1",
        "outputId": "622015b0-133b-4064-8305-c5080e5021d2"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(0.00944467, dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This seems like **a lot of work**, there must be an easier way! And there is one indeed: we can use [**item()**](https://pytorch.org/docs/stable/generated/torch.Tensor.item.html#torch.Tensor.item), for **tensors with a single element** or [**tolist()**](https://pytorch.org/docs/stable/generated/torch.Tensor.tolist.html#torch.Tensor.tolist) otherwise."
      ],
      "metadata": {
        "id": "NMYkkQ2oRnlj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(loss.item(), loss.tolist())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1yfxmlaIRm3F",
        "outputId": "eecab56f-72b3-4aab-ec2b-92cef012bdba"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.009444665163755417 0.009444665163755417\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "drUCGSXmRqVl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}